val rdd = sc.textFile("food_reviews/Reviews.csv")

val rdd1 = rdd.map(x => x.replaceAll("\"\"",""))

rdd1.saveAsTextFile("food_reviews/food_reviews.csv")

val df = spark.read.format("csv").option("quoteAll","true").load("food_reviews/food_reviews.csv").toDF("Id","ProductId","UserId","ProfileName","HelpfulnessNumerator","HelpfulnessDenominator","Score","Time","Summary","Text")
df: org.apache.spark.sql.DataFrame = [Id: string, ProductId: string ... 8 more fields]

df.printSchema
root
 |-- Id: integer (nullable = true)
 |-- ProductId: string (nullable = true)
 |-- UserId: string (nullable = true)
 |-- ProfileName: string (nullable = true)
 |-- HelpfulnessNumerator: string (nullable = true)
 |-- HelpfulnessDenominator: string (nullable = true)
 |-- Score: string (nullable = true)
 |-- Time: string (nullable = true)
 |-- Summary: string (nullable = true)
 |-- Text: string (nullable = true)


df.show
+------+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+
|    Id| ProductId|        UserId|         ProfileName|HelpfulnessNumerator|HelpfulnessDenominator|Score|      Time|             Summary|                Text|
+------+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+
|250074|B0029NII3C|A2830ZHYJP2J1U|            B. Grove|                   0|                     0|    3|1338595200| Normally a favorite|I like the food i...|
|250075|B0029NII3C|A2I6FFCUE6KID9|Stone Maven Stone...|                   0|                     0|    5|1338422400|Picky cat loves this|I have a cat with...|
|250076|B0029NII3C| ASALATXVOO93K|       OC Mom OC Mom|                   0|                     0|    3|1337040000|My cats love this...|My 2 cats are pic...|
|250077|B0029NII3C| AY248XZFR20X8| Trisha Neuschwander|                   0|                     0|    5|1335312000|      Kitty Loves It|I have looked for...|
|250078|B0029NII3C|A3KAB2WMDVS6S3|Doug Kueffler Gru...|                   0|                     0|    5|1334016000|Whiskas Tuna Entr...|After Wal-mart di...|
|250079|B0029NII3C|A19QLFHBU1GXTQ| Dale T. Jenkins DTJ|                   0|                     0|    5|1333497600|My Cat's Favorite...|Sir Flip loves th...|
|250080|B0029NII3C|A3MYCJU91K48FM|Judith A. Reed ju...|                   0|                     0|    5|1332633600|            Cat food|Although serving ...|
|250081|B0029NII3C|A26JYIPTHH25VC|                Moto|                   0|                     0|    5|1331424000|      A cat delicacy|My Kitties(Tuffy ...|
|250082|B0029NII3C|A17LIW1UGS6IKT|      Paul F. Austin|                   0|                     0|    5|1329004800|Our cats love thi...|The Whiskas Purrf...|
|250083|B0029NII3C| ATI8X8CPAU5N6|Eleanor Martin bo...|                   0|                     0|    5|1328486400|      Pleasing Molly|Molly wakes me up...|
|250084|B0029NII3C| AYAH5HYV62EXV|    Connie S. Miller|                   0|                     0|    5|1327622400|Just what The Cat...|The Cat will not ...|
|250085|B0029NII3C|A3IINUJ0L32ZR0|       Barbara Dewey|                   0|                     0|    5|1326672000|           very good|I have a very old...|
|250086|B0029NII3C|A12BTR2MVK2BR5|             xenofan|                   0|                     0|    4|1326412800|my cat prefers th...|My cat is a fussy...|
|250087|B0029NII3C|A1IPZ9ATPDR56O|              G Gina|                   0|                     0|    5|1324425600|        its all good|I have 2 cats who...|
|250088|B0029NII3C| ABQXS84HVNC1N|D. Brenner gift g...|                   0|                     0|    5|1323475200|   Purrfectly great.|My cat was allerg...|
|250089|B0029NII3C|A1NK7L4ZS8CNWH|        R. A. Hoerst|                   0|                     0|    3|1323302400|cat rejected one ...|I like the conven...|
|250090|B0029NII3C|A2LEA8LWF8G2U9|       Meowin' Kitty|                   0|                     0|    5|1321056000|Whiskas Purrfectl...|Exactly what I ne...|
|250091|B0029NII3C| AGVHHNGCXSUJE|            Cat Mama|                   0|                     0|    4|1319155200|Our kitties love ...|We have two very ...|
|250092|B0029NII3C| A1RET8URKV6NV|              Melvin|                   0|                     0|    5|1318896000|My Cat Loves Whis...|Cats can be very ...|
|250093|B0029NII3C|A3P8CU9874SRK5|        C. christine|                   0|                     0|    2|1316649600|Unwanted Ingredients|I was interested ...|
+------+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+
only showing top 20 rows


import org.apache.spark.sql.types._

val df1 = df.where('HelpfulnessNumerator =!= "HelpfulnessNumerator").select(('Score.cast(DoubleType)-1).as("label"), 'Text)
df1: org.apache.spark.sql.DataFrame = [label: double, Text: string]

df1.groupBy("label").count.show
+-----+------+                                                                  
|label| count|
+-----+------+
|  0.0| 52268|
|  1.0| 29769|
|  4.0|363120|
|  3.0| 80655|
|  2.0| 42640|
+-----+------+

import org.apache.spark.ml.feature.RegexTokenizer
val tokenizer = new RegexTokenizer().setInputCol("Text").setOutputCol("words").setPattern("""\W+""")
val df2 = tokenizer.transform(df1)

df2.select(explode('words).as("word")).distinct.count  // 120382

// filter out numbers and tokens that are words mixed with numbers
val filterNumbers = df2.select(explode('words).as("word")).where('word.rlike("^[0-9]*$")).distinct

// lists tokens greather one-character length
val tokenCountsFilteredSize = df2.select(explode('words).as("word")).where(length('word) === 1).distinct

// remove terms with only one-occurrence
val rareTokens = df2.select(explode('words).as("word")).groupBy('word).count.where('count === 1).select('word)

// unioned all terms to be removed
val wholeFilters = filterNumbers.union(tokenCountsFilteredSize).union(rareTokens).distinct.cache

wholeFilters.count  // 50999

wholeFilters.printSchema
root
 |-- word: string (nullable = true)
 
val removedWords = wholeFilters.select("word").map( x => x.getString(0)).collect.toArray
removedWords: Array[String] = Array(2240876, 07, 2235212, 2222814, 2229610, 2293424, 2200840, 2288878, 2200535, 296, 2081310, 2104547, 1955583, 1734557, 1966363, 1970480, 1965893, 1632896, 1596252, 1442175, 1528293, 1588635, 1609594, 1591770, 1937610, 1482904, 1575456, 1527737, 1425137, 1632183, 1516351, 1631447, 1441756, 1424699, 1515207, 1441956, 1527418, 1385384, 1424210, 3127468040, 1322530, 1396135, 1277616, 675, 31106617, 1227875, 1098228, 1154694, 1229790, 525313, 580480, 156277, 164418, 250487, 428783, 63814, 80305, deleterious, maximun, 540lux, laterel, unfinsished, responsibiility, alergent, docmentation, filterts, involving, orvetto, painters, zarlengo, removee, icediinstructed, fog, residoe, ctritical, certifiedmanager, blossom, reorganizeed, asmooth, underneatht, harder, ea...

// remove stop words
import org.apache.spark.ml.feature.StopWordsRemover
val enStopWords = StopWordsRemover.loadDefaultStopWords("english")
val remover = new StopWordsRemover().setStopWords(enStopWords).
setInputCol("words").
setOutputCol("filteredStopWords")
val df3 = remover.transform(df2)

// total words after stopwords removal
df3.select(explode('filteredStopWords).as("word")).distinct.count  // 120251

// remove tokens collected in removedListWords
import org.apache.spark.ml.feature.StopWordsRemover
val remover = new StopWordsRemover().setStopWords(removedWords).
setInputCol("filteredStopWords").
setOutputCol("filtered")
val df4 = remover.transform(df3)

// total words relevant for analysis
df4.select(explode('filtered).as("word")).distinct.count  // 69256

val dim = math.pow(2, 15).toInt  // 131072

import org.apache.spark.ml.feature.HashingTF
val tf = new HashingTF().setInputCol("filtered").
setOutputCol("TFOut").
setNumFeatures(dim)
val df5 = tf.transform(df4)

import org.apache.spark.ml.feature.IDF
val idf = new IDF().setInputCol("TFOut").setOutputCol("features")
val idfModel = idf.fit(df5)
val df6 = idfModel.transform(df5)

df6.printSchema
root
 |-- label: double (nullable = true)
 |-- Text: string (nullable = true)
 |-- words: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- filteredStopWords: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- filtered: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- TFOut: vector (nullable = true)
 |-- features: vector (nullable = true)


val Array(trainingData, testData) = df6.randomSplit(Array(0.7,0.3),11L)

trainingData.cache
trainingData.count  // 397611

---- ML OneVsRest classification --------------

import org.apache.spark.ml.classification.{LogisticRegression, OneVsRest}
val lr = new LogisticRegression
lr.setRegParam(0.01).setMaxIter(200).setFitIntercept(true)

val ovr = new OneVsRest().setClassifier(lr)

val ovrmodel = ovr.fit(trainingData)
val pred = ovrmodel.transform(testData).cache

import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
val evaluator = new MulticlassClassificationEvaluator().setMetricName("accuracy")

evaluator.evaluate(pred)  // 0.7491585743469074

val predRDD = pred.select("prediction","label").rdd.map( row => (row(0).toString.toDouble,row(1).toString.toDouble)).cache

predRDD.take(20)
res13: Array[(Double, Double)] = Array((4.0,0.0), (4.0,0.0), (4.0,0.0), (0.0,0.0), (4.0,0.0), (4.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (4.0,0.0), (4.0,0.0), (4.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (4.0,0.0), (4.0,0.0))

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(predRDD)

predRDD.filter(x => x._1 == x._2).count  // 127987
predRDD.count     // 170841
metrics.accuracy  // 0.7491585743469074

metrics.confusionMatrix
res17: org.apache.spark.mllib.linalg.Matrix =
9954.0  660.0   616.0   529.0   3796.0
1751.0  2521.0  978.0   711.0   3071.0
950.0   595.0   4237.0  1777.0  5108.0
464.0   311.0   1071.0  7838.0  14764.0
875.0   424.0   995.0   3408.0  103437.0

---- ML Naive Bayes classification --------------

import org.apache.spark.ml.classification.NaiveBayes
val model = new NaiveBayes().fit(trainingData)

val pred = model.transform(testData).cache

import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
val evaluator = new MulticlassClassificationEvaluator().setMetricName("accuracy")

evaluator.evaluate(pred)  // 0.6425565291703982

val predRDD = pred.select("prediction","label").rdd.map( row => (row(0).toString.toDouble,row(1).toString.toDouble)).cache

predRDD.take(20)
res6: Array[(Double, Double)] = Array((3.0,6.0), (3.0,6.0), (3.0,6.0), (3.0,6.0), (3.0,6.0), (3.0,6.0), (2.0,1.0), (2.0,1.0), (2.0,1.0), (2.0,1.0), (1.0,1.0), (2.0,1.0), (2.0,1.0), (2.0,1.0), (1.0,1.0), (2.0,1.0), (2.0,1.0), (2.0,1.0), (1.0,1.0), (2.0,1.0))
import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(predRDD)

predRDD.filter(x => x._1 == x._2).count  // 109775
predRDD.count     // 170841
metrics.accuracy  // 0.6425565291703982

metrics.confusionMatrix
res23: org.apache.spark.mllib.linalg.Matrix =
9393.0  2750.0  1294.0  804.0    1314.0
1754.0  3600.0  1601.0  956.0    1121.0
1514.0  1716.0  4976.0  2497.0   1964.0
1348.0  1401.0  2829.0  10598.0  8272.0
4273.0  3102.0  4884.0  15672.0  81208.0