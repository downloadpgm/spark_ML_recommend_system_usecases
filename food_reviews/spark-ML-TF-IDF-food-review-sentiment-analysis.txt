
val df = spark.read.format("csv").option("header","true").option("inferSchema","true").option("quoteAll","true").load("hdfs://471b9f42f61c:9000/data/Reviews.csv")
df: org.apache.spark.sql.DataFrame = [Id: int, ProductId: string ... 8 more fields]

df.printSchema
root
 |-- Id: integer (nullable = true)
 |-- ProductId: string (nullable = true)
 |-- UserId: string (nullable = true)
 |-- ProfileName: string (nullable = true)
 |-- HelpfulnessNumerator: string (nullable = true)
 |-- HelpfulnessDenominator: string (nullable = true)
 |-- Score: string (nullable = true)
 |-- Time: string (nullable = true)
 |-- Summary: string (nullable = true)
 |-- Text: string (nullable = true)


scala> df.show
+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+
| Id| ProductId|        UserId|         ProfileName|HelpfulnessNumerator|HelpfulnessDenominator|Score|      Time|             Summary|                Text|
+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+
|  1|B001E4KFG0|A3SGXH7AUHU8GW|          delmartian|                   1|                     1|    5|1303862400|Good Quality Dog ...|I have bought sev...|
|  2|B00813GRG4|A1D87F6ZCVE5NK|              dll pa|                   0|                     0|    1|1346976000|   Not as Advertised|"Product arrived ...|
|  3|B000LQOCH0| ABXLMWJIXXAIN|"Natalia Corres "...|                   1|                     1|    4|1219017600|"""Delight"" says...|"This is a confec...|
|  4|B000UA0QIQ|A395BORC6FGVXV|                Karl|                   3|                     3|    2|1307923200|      Cough Medicine|If you are lookin...|
|  5|B006K2ZZ7K|A1UQRSCLF8GW1T|"Michael D. Bigha...|                   0|                     0|    5|1350777600|         Great taffy|Great taffy at a ...|
|  6|B006K2ZZ7K| ADT0SRK1MGOEU|      Twoapennything|                   0|                     0|    4|1342051200|          Nice Taffy|I got a wild hair...|
|  7|B006K2ZZ7K|A1SP2KVKFXXRU1|   David C. Sullivan|                   0|                     0|    5|1340150400|Great!  Just as g...|This saltwater ta...|
|  8|B006K2ZZ7K|A3JRGQVEQN31IQ|  Pamela G. Williams|                   0|                     0|    5|1336003200|Wonderful, tasty ...|This taffy is so ...|
|  9|B000E7L2R4|A1MZYO9TZK0BBI|            R. James|                   1|                     1|    5|1322006400|          Yay Barley|Right now I'm mos...|
| 10|B00171APVA|A21BT40VZCCYT4|       Carol A. Reed|                   0|                     0|    5|1351209600|    Healthy Dog Food|This is a very he...|
| 11|B0001PB9FE|A3HDKO7OW0QNK4|        Canadian Fan|                   1|                     1|    5|1107820800|The Best Hot Sauc...|I don't know if i...|
| 12|B0009XLVG0|A2725IB4YY9JEB|"A Poeng ""Sparky...|                   4|                     4|    5|1282867200|"My cats LOVE thi...|One of my boys ne...|
| 13|B0009XLVG0| A327PCT23YH90|                  LT|                   1|                     1|    1|1339545600|My Cats Are Not F...|My cats have been...|
| 14|B001GVISJM|A18ECVX2RJ7HUE| "willie ""roadie"""|                   2|                     2|    4|1288915200|   fresh and greasy!|good flavor! thes...|
| 15|B001GVISJM|A2MUGFV2TDQ47K|"Lynrie ""Oh HELL...|                   4|                     5|    5|1268352000|Strawberry Twizzl...|The Strawberry Tw...|
| 16|B001GVISJM|A1CZX3CP8IKQIJ|        Brian A. Lee|                   4|                     5|    5|1262044800|Lots of twizzlers...|My daughter loves...|
| 17|B001GVISJM|A3KLWF6WQ5BNYO|      Erica Neathery|                   0|                     0|    2|1348099200|          poor taste|I love eating the...|
| 18|B001GVISJM| AFKW14U97Z6QO|               Becca|                   0|                     0|    5|1345075200|            Love it!|I am very satisfi...|
| 19|B001GVISJM|A2A9X58G2GTBLP|             Wolfee1|                   0|                     0|    5|1324598400|  GREAT SWEET CANDY!|Twizzlers, Strawb...|
| 20|B001GVISJM|A3IV7CL2C13K2U|                Greg|                   0|                     0|    5|1318032000|Home delivered tw...|Candy was deliver...|
+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+
only showing top 20 rows


import org.apache.spark.sql.types._

val df1 = df.where('HelpfulnessNumerator =!= "HelpfulnessNumerator").select(('Score.cast(DoubleType)-1).as("label"), 'Text)
df1: org.apache.spark.sql.DataFrame = [label: double, Text: string]

df1.groupBy("label").count.orderBy("label").show
+-----+------+                                                                  
|label| count|
+-----+------+
| null|   290|
| -1.0|  1111|
|  0.0| 52635|
|  1.0| 29877|
|  2.0| 42502|
|  3.0| 80141|
|  4.0|361648|
|  5.0|    38|
|  6.0|    41|
|  7.0|    12|
|  8.0|    22|
|  9.0|    21|
| 10.0|     7|
| 11.0|     5|
| 12.0|     3|
| 13.0|    10|
| 14.0|     6|
| 15.0|     5|
| 16.0|     3|
| 17.0|     3|
+-----+------+
only showing top 20 rows

val df1a = df1.where("label between 0 and 4")

df1a.groupBy("label").count.show
+-----+------+                                                                  
|label| count|
+-----+------+
|  0.0| 52635|
|  1.0| 29877|
|  4.0|361648|
|  3.0| 80141|
|  2.0| 42502|
+-----+------+


import org.apache.spark.ml.feature.RegexTokenizer
val tokenizer = new RegexTokenizer().setInputCol("Text").setOutputCol("words").setPattern("""\W+""")
val df2 = tokenizer.transform(df1a)

df2.select(explode('words).as("word")).distinct.count  // 109246

// filter out numbers and tokens that are words mixed with numbers
val filterNumbers = df2.select(explode('words).as("word")).where('word.rlike("^[0-9]*$")).distinct

// lists tokens greather one-character length
val tokenCountsFilteredSize = df2.select(explode('words).as("word")).where(length('word) === 1).distinct

// remove terms with only one-occurrence
val rareTokens = df2.select(explode('words).as("word")).groupBy('word).count.where('count === 1).select('word)

// unioned all terms to be removed
val wholeFilters = filterNumbers.union(tokenCountsFilteredSize).union(rareTokens).distinct.cache

wholeFilters.count  // 46403

wholeFilters.printSchema
root
 |-- word: string (nullable = true)
 
val removedWords = wholeFilters.select("word").map( x => x.getString(0)).collect.toArray
removedWords: Array[String] = Array(07, 44032, 38672, 38271, 829, 1603425462, 1266624000, 1090, 296, yesterdy, amplifier, b000e5ao8o, b001eo5wo0, jettison, yeeeuuukkkkk, b000087l6t, immarco, shihzhu, undesireably, navrattan, b000qv0m2e, afnkjganfpbgneonb, fuelbelt, consistcy, briefest, imprerssed, b0009vzp6y, respectably, b000cr1vsq, reforestation, solaced, everyine, singling, sidelined, lieutenant, paramedics, treatnent, inertial, roundabout, ipad2, undoctored, suzhou, mininal, squeezmo, syked, oleta, amyy, petrushka, darljeeling, mosy, ingenuetea, brothiness, transference, peche, vegweb, barware, affeciando, xlyo, brocher, distancing, aked, untripped, candelled, b001eppyfq, execption, twixed, unavailble, noosing, wrotten, commerceial, workut, stupefaction, mooney, firesale, mesuring, ...

// remove stop words
import org.apache.spark.ml.feature.StopWordsRemover
val enStopWords = StopWordsRemover.loadDefaultStopWords("english")
val remover = new StopWordsRemover().setStopWords(enStopWords).
setInputCol("words").
setOutputCol("filteredStopWords")
val df3 = remover.transform(df2)

// total words after stopwords removal
df3.select(explode('filteredStopWords).as("word")).distinct.count  // 109115

// remove tokens collected in removedListWords
import org.apache.spark.ml.feature.StopWordsRemover
val remover = new StopWordsRemover().setStopWords(removedWords).
setInputCol("filteredStopWords").
setOutputCol("filtered")
val df4 = remover.transform(df3)

// total words relevant for analysis
df4.select(explode('filtered).as("word")).distinct.count  // 62716

val dim = math.pow(2, 15).toInt  // 131072

import org.apache.spark.ml.feature.HashingTF
val tf = new HashingTF().setInputCol("filtered").
setOutputCol("TFOut").
setNumFeatures(dim)
val df5 = tf.transform(df4)

import org.apache.spark.ml.feature.IDF
val idf = new IDF().setInputCol("TFOut").setOutputCol("features")
val idfModel = idf.fit(df5)
val df6 = idfModel.transform(df5)

df6.printSchema
root
 |-- label: double (nullable = true)
 |-- Text: string (nullable = true)
 |-- words: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- filteredStopWords: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- filtered: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- TFOut: vector (nullable = true)
 |-- features: vector (nullable = true)


val Array(trainingData, testData) = df6.randomSplit(Array(0.7,0.3),11L)

trainingData.cache
trainingData.count  // 397611

---- ML OneVsRest classification --------------

import org.apache.spark.ml.classification.{LogisticRegression, OneVsRest}
val lr = new LogisticRegression
lr.setRegParam(0.01).setMaxIter(100).setFitIntercept(true)

val ovr = new OneVsRest().setClassifier(lr)

val ovrmodel = ovr.fit(trainingData)
val pred = ovrmodel.transform(testData).cache

import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
val evaluator = new MulticlassClassificationEvaluator().setMetricName("accuracy")

evaluator.evaluate(pred)  // 0.7491585743469074

val predRDD = pred.select("prediction","label").rdd.map( row => (row(0).toString.toDouble,row(1).toString.toDouble)).cache

predRDD.take(20)
res13: Array[(Double, Double)] = Array((4.0,0.0), (4.0,0.0), (4.0,0.0), (0.0,0.0), (4.0,0.0), (4.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (4.0,0.0), (4.0,0.0), (4.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (4.0,0.0), (4.0,0.0))

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(predRDD)

predRDD.filter(x => x._1 == x._2).count  // 127987
predRDD.count     // 170841
metrics.accuracy  // 0.7491585743469074

metrics.confusionMatrix
res17: org.apache.spark.mllib.linalg.Matrix =
9954.0  660.0   616.0   529.0   3796.0
1751.0  2521.0  978.0   711.0   3071.0
950.0   595.0   4237.0  1777.0  5108.0
464.0   311.0   1071.0  7838.0  14764.0
875.0   424.0   995.0   3408.0  103437.0

---- ML Naive Bayes classification --------------

import org.apache.spark.ml.classification.NaiveBayes
val model = new NaiveBayes().fit(trainingData)

val pred = model.transform(testData).cache

import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
val evaluator = new MulticlassClassificationEvaluator().setMetricName("accuracy")

evaluator.evaluate(pred)  // 0.6425565291703982

val predRDD = pred.select("prediction","label").rdd.map( row => (row(0).toString.toDouble,row(1).toString.toDouble)).cache

predRDD.take(20)
res6: Array[(Double, Double)] = Array((3.0,6.0), (3.0,6.0), (3.0,6.0), (3.0,6.0), (3.0,6.0), (3.0,6.0), (2.0,1.0), (2.0,1.0), (2.0,1.0), (2.0,1.0), (1.0,1.0), (2.0,1.0), (2.0,1.0), (2.0,1.0), (1.0,1.0), (2.0,1.0), (2.0,1.0), (2.0,1.0), (1.0,1.0), (2.0,1.0))
import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(predRDD)

predRDD.filter(x => x._1 == x._2).count  // 109775
predRDD.count     // 170841
metrics.accuracy  // 0.6425565291703982

metrics.confusionMatrix
res23: org.apache.spark.mllib.linalg.Matrix =
9393.0  2750.0  1294.0  804.0    1314.0
1754.0  3600.0  1601.0  956.0    1121.0
1514.0  1716.0  4976.0  2497.0   1964.0
1348.0  1401.0  2829.0  10598.0  8272.0
4273.0  3102.0  4884.0  15672.0  81208.0